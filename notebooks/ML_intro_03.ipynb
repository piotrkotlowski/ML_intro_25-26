{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b12aa94f",
   "metadata": {},
   "source": [
    "# Wstęp do analizy danych i uczenia maszynowego\n",
    "## 3. Regresja liniowa, pipeliny w scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c146cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d731f378",
   "metadata": {},
   "source": [
    "W zbiorze danych `mpg` (miles per gallon) z biblioteki seaborn znajdują się informacje o samochodach, w tym ich zużycie paliwa (mpg), które chcemy przewidzieć na podstawie innych cech pojazdów.\n",
    "\n",
    "Na początku przyjrzmy się temu zbiorowi danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781a888a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sns.load_dataset(\"mpg\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23033b64",
   "metadata": {},
   "source": [
    "Niektóre ze zmiennych są kategoryczne, przed stworzeniem modelu będziemy musieli je zakodować (np. za pomocą one-hot encoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660868bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43d5901",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f3b24d",
   "metadata": {},
   "source": [
    "W jednej z kolumn pojawiają się wartości brakujące, które również musimy obsłużyć przed trenowaniem modelu.\n",
    "\n",
    "Przed przetwarzaniem danych podzielmy je na zbiór treningowy i testowy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea34c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"mpg\", axis=1)\n",
    "y = df[\"mpg\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb0c6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(), X_test.head(), y_train.head(), y_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24754603",
   "metadata": {},
   "source": [
    "W sprawnym przetwarzaniu danych i trenowaniu modeli w scikit-learn bardzo pomocne są pipeliny. Umożliwiają one łączenie wielu kroków przetwarzania danych i trenowania modelu w jeden obiekt, co znacznie upraszcza kod.\n",
    "\n",
    "Na początku zdefiniujemy, które ze zmiennych są kategoryczne, a które numeryczne. Następnie stworzymy osobne przetwarzania dla każdej z tych grup zmiennych, a na końcu połączymy je w jeden pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdb52ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "cat_features = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "print(\"Wszystkie cechy:\", X_train.columns.tolist())\n",
    "print(\"Numeryczne cechy:\", num_features)\n",
    "print(\"Kategoryczne cechy:\", cat_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6159bd1",
   "metadata": {},
   "source": [
    "W ramach przetwarzania danych dla zmiennych numerycznych uzupełnimy brakujące wartości medianą i standaryzujemy cechy za pomocą `StandardScaler`. Dla zmiennych kategorycznych uzupełnimy brakujące wartości najczęściej występującą kategorią i zakodujemy je za pomocą `OneHotEncoder`.\n",
    "\n",
    "Po więcej szczegółów odnośnie modułu `Pipeline` i `ColumnTransformer` odsyłam do dokumentacji scikit-learn:\n",
    "- [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
    "- [ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9435cb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"num\", num_pipeline, num_features),\n",
    "    (\"cat\", cat_pipeline, cat_features)\n",
    "])\n",
    "\n",
    "data_pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor)\n",
    "])\n",
    "\n",
    "# Dopasowanie pipeline do danych treningowych\n",
    "data_pipeline.fit(X_train)\n",
    "\n",
    "# Użycie pipeline do utworzenia przetworzonych ramek danych\n",
    "feature_names = (\n",
    "    pd.Index(data_pipeline.named_steps['preprocessor'].get_feature_names_out())\n",
    "    .str.replace(\"num__\", \"\", regex=False)\n",
    "    .str.replace(\"cat__\", \"\", regex=False)\n",
    ")\n",
    "\n",
    "X_train_processed = pd.DataFrame(data_pipeline.transform(X_train).toarray(),\n",
    "                                columns=feature_names)\n",
    "\n",
    "X_test_processed = pd.DataFrame(data_pipeline.transform(X_test).toarray(),\n",
    "                                columns=feature_names)\n",
    "\n",
    "X_train_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94dd0ff",
   "metadata": {},
   "source": [
    "Otrzymaliśmy w ten sposób nowy, gotowy do trenowania modelu zbiór cech. Teraz możemy stworzyć i wytrenować model regresji liniowej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb6dcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(positive=False)\n",
    "\n",
    "model.fit(X_train_processed, y_train)\n",
    "y_pred = model.predict(X_test_processed)\n",
    "mse_train = mean_squared_error(y_train, model.predict(X_train_processed))\n",
    "r2_train = r2_score(y_train, model.predict(X_train_processed))\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "r2_test = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Linear Regression - Mean Squared Error, Train: {mse_train:.2f}, Test: {mse_test:.2f}\")\n",
    "print(f\"Linear Regression - R^2 Score, Train: {r2_train:.2f}, Test: {r2_test:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581bbda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lasso = Lasso(alpha=1)\n",
    "\n",
    "model_lasso.fit(X_train_processed, y_train)\n",
    "y_pred_lasso = model_lasso.predict(X_test_processed)\n",
    "mse_lasso_train = mean_squared_error(y_train, model_lasso.predict(X_train_processed))\n",
    "r2_lasso_train = r2_score(y_train, model_lasso.predict(X_train_processed))\n",
    "mse_lasso_test = mean_squared_error(y_test, y_pred_lasso)\n",
    "r2_lasso_test = r2_score(y_test, y_pred_lasso)\n",
    "\n",
    "print(f\"Lasso Regression - Mean Squared Error, Train: {mse_lasso_train:.2f}, Test: {mse_lasso_test:.2f}\")\n",
    "print(f\"Lasso Regression - R^2 Score, Train: {r2_lasso_train:.2f}, Test: {r2_lasso_test:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9720407",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ridge = Ridge(alpha=1)\n",
    "\n",
    "model_ridge.fit(X_train_processed, y_train)\n",
    "y_pred_ridge = model_ridge.predict(X_test_processed)\n",
    "mse_ridge_train = mean_squared_error(y_train, model_ridge.predict(X_train_processed))\n",
    "r2_ridge_train = r2_score(y_train, model_ridge.predict(X_train_processed))\n",
    "mse_ridge_test = mean_squared_error(y_test, y_pred_ridge)\n",
    "r2_ridge_test = r2_score(y_test, y_pred_ridge)\n",
    "\n",
    "print(f\"Ridge Regression - Mean Squared Error, Train: {mse_ridge_train:.2f}, Test: {mse_ridge_test:.2f}\")\n",
    "print(f\"Ridge Regression - R^2 Score, Train: {r2_ridge_train:.2f}, Test: {r2_ridge_test:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94c28c7",
   "metadata": {},
   "source": [
    "W zwykłej regresji na zbiorze testowym dzieje się coś dziwnego, zignorujmy to na razie, nie rozumiem skąd to się bierze.\n",
    "\n",
    "Modele Lasso i Ridge dają sensowne metryki zarówno na zbiorze treningowym jak i testowym, przy czym Lasso ma podobne wartości na obu zbiorach, a Ridge na istotnie lepsze na zbiorze treningowym niż testowym, co może sugerować lekkie przeuczenie.\n",
    "\n",
    "Spróbujmy stworzyć model Ridge z większym parametrem regularizacji alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03d78d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ridge = Ridge(alpha=100)\n",
    "\n",
    "model_ridge.fit(X_train_processed, y_train)\n",
    "y_pred_ridge = model_ridge.predict(X_test_processed)\n",
    "mse_ridge_train = mean_squared_error(y_train, model_ridge.predict(X_train_processed))\n",
    "r2_ridge_train = r2_score(y_train, model_ridge.predict(X_train_processed))\n",
    "mse_ridge_test = mean_squared_error(y_test, y_pred_ridge)\n",
    "r2_ridge_test = r2_score(y_test, y_pred_ridge)\n",
    "\n",
    "print(f\"Ridge Regression - Mean Squared Error, Train: {mse_ridge_train:.2f}, Test: {mse_ridge_test:.2f}\")\n",
    "print(f\"Ridge Regression - R^2 Score, Train: {r2_ridge_train:.2f}, Test: {r2_ridge_test:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f94eb9",
   "metadata": {},
   "source": [
    "Przy dużej wartości alpha model Ridge osiąga podobne wyniki na zbiorze treningowym i testowym, co sugeruje, że model nie jest już przeuczony.\n",
    "\n",
    "Zwrómy jeszcze uwagę na współczynniki zmiennych w modelu Lasso, tylko dwie cechy mają niezerowe współczynniki, co oznacza, że model Lasso wybrał tylko te dwie cechy jako istotne dla przewidywania zużycia paliwa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3f81f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_coefs = pd.Series(model_lasso.coef_, index=X_train_processed.columns)\n",
    "print(\"Lasso Coefficients:\")\n",
    "print(lasso_coefs.sort_values(ascending=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
